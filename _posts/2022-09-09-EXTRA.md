---
title: '论文笔记：EXTRA:An Exact first-order algorithm for decentralized consensus optimization'
date: 2022-09-09
permalink: /posts/2022/07/blog-post-EXTRA/
tags:
  - convex optimization
  - decentralized optimization
---

近几天在学习分布式优化，这里对我从参考书和论文中的算法设计历史脉络以及open problems做一个整理

# Decentralized Gradient Descent (DGD)

DGD算法的一般形式如下：

$$x^{(k+1)} = W x^{(k)} - \alpha^k \nabla f(x^{(k)})$$

DGD算法本质上是将优化过程分成了两部分：一部分是consensus,即利用Weight matrix将连通节点的信息做一个沟通；另一部分就是传统的梯度下降，这里的梯度下降是针对每一个local节点。而DGD有一个显著的缺点：如果DGD当中的步长选择常数的话，那么会得到inexact convergence；而如果选择逐渐趋于零的步长(diminishing step size)，那么虽然可以得到exact convergence, 但是会造成较慢的收敛速度，这在实际应用当中是一个棘手的问题。

# Decentrailized Stochastic Gradient Descent (DSGD)

而在中心化算法当中效果很好的SGD也能在分布式优化当中应用起来，他的一般更新形式如下：

$$x^{(k+1)} = W x^{(k)} - \alpha^k \nabla f_i(x^{(k)})$$

而在SGD中为了获得较好的收敛效果，通常我们会做一个bounded variance的假设。

# EXTRA: An Exact first-order algorithm for decentralized consensus optimization


